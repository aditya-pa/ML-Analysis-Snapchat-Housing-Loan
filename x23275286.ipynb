{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b6c487b-948f-4bae-b373-d6ffda7defbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/b9/rs9ddddj2qb9xx5yj7h2b03w0000gn/T/236493e067204fdcbd0a8e215d8e5000-pulp.mps -max -timeMode elapsed -branch -printingOptions all -solution /var/folders/b9/rs9ddddj2qb9xx5yj7h2b03w0000gn/T/236493e067204fdcbd0a8e215d8e5000-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 85 COLUMNS\n",
      "At line 457 RHS\n",
      "At line 538 BOUNDS\n",
      "At line 603 ENDATA\n",
      "Problem MODEL has 80 rows, 72 columns and 236 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 1800 - 0.00 seconds\n",
      "Cgl0003I 7 fixed, 0 tightened bounds, 0 strengthened rows, 0 substitutions\n",
      "Cgl0004I processed model has 22 rows, 17 columns (9 integer (9 of which binary)) and 53 elements\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cuts at root node changed objective from 1.79769e+308 to -1.79769e+308\n",
      "Probing was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Linear relaxation infeasible\n",
      "\n",
      "No feasible solution found\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.00\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.02\n",
      "\n",
      "Visited Cities: ['A', 'D', 'E', 'H']\n",
      "Optimal Delivery Value: 1800.0\n",
      "Arrival Times:\n",
      "A: 0.0\n",
      "D: 11.0\n",
      "E: 13.0\n",
      "H: 14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pulp import LpProblem, LpVariable, lpSum, LpMaximize\n",
    "\n",
    "# The original generate_distances function remains unchanged\n",
    "def generate_distances(num_cities, seed):\n",
    "    np.random.seed(seed)  # Set the seed for reproducibility\n",
    "    # Create a random distance matrix (lower triangle only for symmetry)\n",
    "    distances = np.random.randint(1, 1000, size=(num_cities, num_cities))\n",
    "    # Make the matrix symmetric\n",
    "    distances = np.triu(distances, 1) + np.triu(distances, 1).T\n",
    "    # Set the diagonal to 0 (distance from a city to itself)\n",
    "    np.fill_diagonal(distances, 0)\n",
    "    return distances\n",
    "\n",
    "# Function to calculate travel times from the distance matrix\n",
    "def calculate_travel_times(distance_matrix, speed):\n",
    "    return distance_matrix / speed\n",
    "\n",
    "# Function to setup the optimization problem\n",
    "def setup_optimization_model(cities, num_cities, delivery_values, travel_times, time_windows):\n",
    "    model = LpProblem(\"TSP_Single_Objective\", LpMaximize)\n",
    "\n",
    "    # Decision Variables\n",
    "    visit = LpVariable.dicts(\"Visit\", range(num_cities), cat=\"Binary\")\n",
    "    travel = LpVariable.dicts(\"Travel\", [(i, j) for i in range(num_cities) for j in range(num_cities)], cat=\"Binary\")\n",
    "    arrival_time = LpVariable.dicts(\"ArrivalTime\", range(num_cities), lowBound=0, cat=\"Continuous\")\n",
    "\n",
    "    # Objective Function: Maximize total delivery value\n",
    "    model += lpSum(delivery_values[i] * visit[i] for i in range(num_cities)), \"Total_Delivery_Value\"\n",
    "\n",
    "    # Constraints\n",
    "    # 1. Start and end at city 0 (index 0)\n",
    "    model += visit[0] == 1, \"Start_at_0\"\n",
    "    model += lpSum(travel[(0, j)] for j in range(1, num_cities)) == 1, \"Leave_0_Once\"\n",
    "    model += lpSum(travel[(i, 0)] for i in range(1, num_cities)) == 1, \"Return_to_0\"\n",
    "\n",
    "    # 2. Visit between 3 and 5 cities\n",
    "    model += lpSum(visit[i] for i in range(num_cities)) >= 3, \"Minimum_Visits\"\n",
    "    model += lpSum(visit[i] for i in range(num_cities)) <= 5, \"Maximum_Visits\"\n",
    "\n",
    "    # 3. Dependency: If city 1 (B) is visited, city 5 (F) must be visited\n",
    "    model += visit[1] <= visit[5], \"Dependency_1_to_5\"\n",
    "\n",
    "    # 4. Mutual Exclusion: Either city 2 (C) or 3 (D) can be visited, not both\n",
    "    model += visit[2] + visit[3] <= 1, \"Mutual_Exclusion_2_3\"\n",
    "\n",
    "    # 5. Mutual Exclusion: Either city 6 (G) or 7 (H) can be visited, not both\n",
    "    model += visit[6] + visit[7] <= 1, \"Mutual_Exclusion_6_7\"\n",
    "\n",
    "    # 6. Time window constraints\n",
    "    for i in range(num_cities):\n",
    "        model += arrival_time[i] >= time_windows[i][0] * visit[i], f\"TimeWindowStart_{i}\"\n",
    "        model += arrival_time[i] <= time_windows[i][1] * visit[i], f\"TimeWindowEnd_{i}\"\n",
    "\n",
    "    # 7. Sequential travel constraints: Arrival times must respect travel durations\n",
    "    M = 1e6  # A large constant for linearization\n",
    "    for i in range(num_cities):\n",
    "        for j in range(num_cities):\n",
    "            if i != j:\n",
    "                model += (\n",
    "                    arrival_time[j]\n",
    "                    >= arrival_time[i] + travel_times[i, j] - M * (1 - travel[(i, j)])\n",
    "                ), f\"TravelTime_{i}_to_{j}\"\n",
    "\n",
    "    return model, visit, arrival_time\n",
    "\n",
    "# Function to print results\n",
    "def print_results(model, cities, visit, arrival_time):\n",
    "    model.solve()\n",
    "\n",
    "    visited_cities = [cities[i] for i in range(len(cities)) if visit[i].value() == 1]\n",
    "    optimal_value = model.objective.value()\n",
    "\n",
    "    print(\"Visited Cities:\", visited_cities)\n",
    "    print(\"Optimal Delivery Value:\", optimal_value)\n",
    "    print(\"Arrival Times:\")\n",
    "    for i in range(len(cities)):\n",
    "        if visit[i].value() == 1:\n",
    "            print(f\"{cities[i]}: {arrival_time[i].value()}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define cities, delivery values, and time windows\n",
    "    cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    num_cities = len(cities)\n",
    "\n",
    "    delivery_values = [0, 500, 300, 400, 600, 200, 350, 450]\n",
    "    time_windows = [(0, 24), (9, 12), (10, 14), (11, 13), (13, 17), (8.5, 10.5), (12, 16), (14, 18)]\n",
    "\n",
    "    # Generate distance matrix and calculate travel times\n",
    "    seed = 5286\n",
    "    distance_matrix = generate_distances(num_cities, seed)\n",
    "    speed = 30  # km/h\n",
    "    travel_times = calculate_travel_times(distance_matrix, speed)\n",
    "\n",
    "    # Set up the optimization model\n",
    "    model, visit, arrival_time = setup_optimization_model(cities, num_cities, delivery_values, travel_times, time_windows)\n",
    "\n",
    "    # Display the results\n",
    "    print_results(model, cities, visit, arrival_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd791978-e346-41bc-9ed2-c367976efaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pulp/solverdir/cbc/osx/64/cbc /var/folders/b9/rs9ddddj2qb9xx5yj7h2b03w0000gn/T/315361f0c5414c62ae605b2c6eed3127-pulp.mps -max -timeMode elapsed -branch -printingOptions all -solution /var/folders/b9/rs9ddddj2qb9xx5yj7h2b03w0000gn/T/315361f0c5414c62ae605b2c6eed3127-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 85 COLUMNS\n",
      "At line 513 RHS\n",
      "At line 594 BOUNDS\n",
      "At line 659 ENDATA\n",
      "Problem MODEL has 80 rows, 72 columns and 236 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Continuous objective value is 888.8 - 0.00 seconds\n",
      "Cgl0003I 9 fixed, 0 tightened bounds, 0 strengthened rows, 0 substitutions\n",
      "Cgl0004I processed model has 17 rows, 14 columns (7 integer (7 of which binary)) and 41 elements\n",
      "Cbc0038I Initial state - 3 integers unsatisfied sum - 1.00002\n",
      "Cbc0038I Pass   1: suminf.    1.00002 (3) obj. -888.8 iterations 2\n",
      "Cbc0038I Pass   2: suminf.    0.00002 (1) obj. -838.8 iterations 3\n",
      "Cbc0038I Pass   3: suminf.    0.00002 (1) obj. -838.8 iterations 0\n",
      "Cbc0038I Pass   4: suminf.    0.00002 (1) obj. -838.8 iterations 0\n",
      "Cbc0038I Pass   5: suminf.    0.00001 (1) obj. -837.933 iterations 6\n",
      "Cbc0038I Pass   6: suminf.    0.00001 (1) obj. -837.933 iterations 0\n",
      "Cbc0038I Pass   7: suminf.    0.00001 (1) obj. -837.933 iterations 0\n",
      "Cbc0038I Pass   8: suminf.    0.00001 (1) obj. -837.933 iterations 0\n",
      "Cbc0038I Pass   9: suminf.    0.29964 (1) obj. -837.674 iterations 3\n",
      "Cbc0038I Pass  10: suminf.    0.35020 (2) obj. -538.237 iterations 7\n",
      "Cbc0038I Pass  11: suminf.    0.00002 (1) obj. -538.8 iterations 3\n",
      "Cbc0038I Pass  12: suminf.    0.00002 (1) obj. -538.8 iterations 0\n",
      "Cbc0038I Pass  13: suminf.    0.00002 (1) obj. -538.8 iterations 0\n",
      "Cbc0038I Pass  14: suminf.    0.35020 (2) obj. -838.237 iterations 3\n",
      "Cbc0038I Pass  15: suminf.    0.00002 (1) obj. -838.8 iterations 3\n",
      "Cbc0038I Pass  16: suminf.    0.00002 (1) obj. -838.8 iterations 0\n",
      "Cbc0038I Pass  17: suminf.    0.45519 (3) obj. -722.396 iterations 8\n",
      "Cbc0038I Pass  18: suminf.    0.03228 (2) obj. -712.961 iterations 3\n",
      "Cbc0038I Pass  19: suminf.    0.00002 (1) obj. -713.8 iterations 2\n",
      "Cbc0038I Pass  20: suminf.    0.00002 (1) obj. -713.8 iterations 0\n",
      "Cbc0038I Pass  21: suminf.    0.00002 (1) obj. -713.8 iterations 0\n",
      "Cbc0038I Pass  22: suminf.    0.00002 (1) obj. -713.8 iterations 0\n",
      "Cbc0038I Pass  23: suminf.    0.03228 (2) obj. -712.961 iterations 4\n",
      "Cbc0038I Pass  24: suminf.    1.03228 (4) obj. -837.961 iterations 3\n",
      "Cbc0038I Pass  25: suminf.    0.00002 (1) obj. -763.8 iterations 4\n",
      "Cbc0038I Pass  26: suminf.    0.00002 (1) obj. -763.8 iterations 0\n",
      "Cbc0038I Pass  27: suminf.    0.00002 (1) obj. -462.933 iterations 6\n",
      "Cbc0038I Pass  28: suminf.    0.00002 (1) obj. -462.933 iterations 0\n",
      "Cbc0038I Pass  29: suminf.    0.00002 (1) obj. -462.933 iterations 0\n",
      "Cbc0038I Pass  30: suminf.    0.35717 (2) obj. -712.68 iterations 4\n",
      "Cbc0038I No solution found this major pass\n",
      "Cbc0038I Before mini branch and bound, 0 integers at bound fixed and 0 continuous\n",
      "Cbc0038I Full problem 17 rows 14 columns, reduced to 17 rows 14 columns\n",
      "Cbc0038I Mini branch and bound did not improve solution (0.01 seconds)\n",
      "Cbc0038I Full problem 18 rows 14 columns, reduced to 17 rows 14 columns\n",
      "Cbc0038I After 0.01 seconds - Feasibility pump exiting - took 0.00 seconds\n",
      "Cbc0006I The LP relaxation is infeasible or too expensive\n",
      "Cbc0013I At root node, 0 cuts changed objective from -888.79998 to -888.79998 in 1 passes\n",
      "Cbc0014I Cut generator 0 (Probing) - 1 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is 1\n",
      "Cbc0014I Cut generator 1 (Gomory) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 2 (Knapsack) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 3 (Clique) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 4 (MixedIntegerRounding2) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 5 (FlowCover) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 6 (TwoMirCuts) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0014I Cut generator 7 (ZeroHalf) - 0 row cuts average 0.0 elements, 0 column cuts (0 active)  in 0.000 seconds - new frequency is -100\n",
      "Cbc0001I Search completed - best objective 1e+50, took 0 iterations and 0 nodes (0.01 seconds)\n",
      "Cbc0035I Maximum depth 0, 0 variables fixed on reduced cost\n",
      "Cuts at root node changed objective from -888.8 to -888.8\n",
      "Probing was tried 1 times and created 1 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Gomory was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Knapsack was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "Clique was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "MixedIntegerRounding2 was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "FlowCover was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "TwoMirCuts was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "ZeroHalf was tried 0 times and created 0 cuts of which 0 were active after adding rounds of cuts (0.000 seconds)\n",
      "\n",
      "Result - Problem proven infeasible\n",
      "\n",
      "No feasible solution found\n",
      "Enumerated nodes:               0\n",
      "Total iterations:               0\n",
      "Time (CPU seconds):             0.00\n",
      "Time (Wallclock seconds):       0.01\n",
      "\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.01   (Wallclock seconds):       0.02\n",
      "\n",
      "Visited Cities: ['A', 'D', 'E', 'H']\n",
      "Optimal Objective Value (Profit - Distance): 888.7999805866667\n",
      "Arrival Times:\n",
      "A: 0.0\n",
      "D: 11.0\n",
      "E: 13.0\n",
      "H: 14.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pulp import LpProblem, LpVariable, lpSum, LpMaximize\n",
    "\n",
    "# The original generate_distances function remains unchanged\n",
    "def generate_distances(num_cities, seed):\n",
    "    np.random.seed(seed)  # Set the seed for reproducibility\n",
    "    # Create a random distance matrix (lower triangle only for symmetry)\n",
    "    distances = np.random.randint(1, 1000, size=(num_cities, num_cities))\n",
    "    # Make the matrix symmetric\n",
    "    distances = np.triu(distances, 1) + np.triu(distances, 1).T\n",
    "    # Set the diagonal to 0 (distance from a city to itself)\n",
    "    np.fill_diagonal(distances, 0)\n",
    "    return distances\n",
    "\n",
    "# Function to calculate travel times from the distance matrix\n",
    "def calculate_travel_times(distance_matrix, speed):\n",
    "    return distance_matrix / speed\n",
    "\n",
    "# Function to setup the optimization model with two objectives\n",
    "def setup_optimization_model(cities, num_cities, delivery_values, travel_times, time_windows, weight_profit=0.5, weight_distance=0.5):\n",
    "    model = LpProblem(\"TSP_Multi_Objective\", LpMaximize)\n",
    "\n",
    "    # Decision Variables\n",
    "    visit = LpVariable.dicts(\"Visit\", range(num_cities), cat=\"Binary\")\n",
    "    travel = LpVariable.dicts(\"Travel\", [(i, j) for i in range(num_cities) for j in range(num_cities)], cat=\"Binary\")\n",
    "    arrival_time = LpVariable.dicts(\"ArrivalTime\", range(num_cities), lowBound=0, cat=\"Continuous\")\n",
    "\n",
    "    # First Objective Function: Maximize total delivery value (profit)\n",
    "    profit_objective = lpSum(delivery_values[i] * visit[i] for i in range(num_cities))\n",
    "\n",
    "    # Second Objective Function: Minimize total distance traveled\n",
    "    distance_objective = lpSum(travel_times[i, j] * travel[(i, j)] for i in range(num_cities) for j in range(num_cities) if i != j)\n",
    "\n",
    "    # Combine both objectives using weighted sum\n",
    "    model += weight_profit * profit_objective - weight_distance * distance_objective, \"Total_Objective\"\n",
    "\n",
    "    # Constraints\n",
    "    # 1. Start and end at city 0 (index 0)\n",
    "    model += visit[0] == 1, \"Start_at_0\"\n",
    "    model += lpSum(travel[(0, j)] for j in range(1, num_cities)) == 1, \"Leave_0_Once\"\n",
    "    model += lpSum(travel[(i, 0)] for i in range(1, num_cities)) == 1, \"Return_to_0\"\n",
    "\n",
    "    # 2. Visit between 3 and 5 cities\n",
    "    model += lpSum(visit[i] for i in range(num_cities)) >= 3, \"Minimum_Visits\"\n",
    "    model += lpSum(visit[i] for i in range(num_cities)) <= 5, \"Maximum_Visits\"\n",
    "\n",
    "    # 3. Dependency: If city 1 (B) is visited, city 5 (F) must be visited\n",
    "    model += visit[1] <= visit[5], \"Dependency_1_to_5\"\n",
    "\n",
    "    # 4. Mutual Exclusion: Either city 2 (C) or 3 (D) can be visited, not both\n",
    "    model += visit[2] + visit[3] <= 1, \"Mutual_Exclusion_2_3\"\n",
    "\n",
    "    # 5. Mutual Exclusion: Either city 6 (G) or 7 (H) can be visited, not both\n",
    "    model += visit[6] + visit[7] <= 1, \"Mutual_Exclusion_6_7\"\n",
    "\n",
    "    # 6. Time window constraints\n",
    "    for i in range(num_cities):\n",
    "        model += arrival_time[i] >= time_windows[i][0] * visit[i], f\"TimeWindowStart_{i}\"\n",
    "        model += arrival_time[i] <= time_windows[i][1] * visit[i], f\"TimeWindowEnd_{i}\"\n",
    "\n",
    "    # 7. Sequential travel constraints: Arrival times must respect travel durations\n",
    "    M = 1e6  # A large constant for linearization\n",
    "    for i in range(num_cities):\n",
    "        for j in range(num_cities):\n",
    "            if i != j:\n",
    "                model += (\n",
    "                    arrival_time[j]\n",
    "                    >= arrival_time[i] + travel_times[i, j] - M * (1 - travel[(i, j)])\n",
    "                ), f\"TravelTime_{i}_to_{j}\"\n",
    "\n",
    "    return model, visit, arrival_time, profit_objective, distance_objective\n",
    "\n",
    "# Function to print results\n",
    "def print_results(model, cities, visit, arrival_time):\n",
    "    model.solve()\n",
    "\n",
    "    visited_cities = [cities[i] for i in range(len(cities)) if visit[i].value() == 1]\n",
    "    optimal_value = model.objective.value()\n",
    "\n",
    "    print(\"Visited Cities:\", visited_cities)\n",
    "    print(\"Optimal Objective Value (Profit - Distance):\", optimal_value)\n",
    "    print(\"Arrival Times:\")\n",
    "    for i in range(len(cities)):\n",
    "        if visit[i].value() == 1:\n",
    "            print(f\"{cities[i]}: {arrival_time[i].value()}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define cities, delivery values, and time windows\n",
    "    cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    num_cities = len(cities)\n",
    "\n",
    "    delivery_values = [0, 500, 300, 400, 600, 200, 350, 450]\n",
    "    time_windows = [(0, 24), (9, 12), (10, 14), (11, 13), (13, 17), (8.5, 10.5), (12, 16), (14, 18)]\n",
    "\n",
    "    # Generate distance matrix and calculate travel times\n",
    "    seed = 5286\n",
    "    distance_matrix = generate_distances(num_cities, seed)\n",
    "    speed = 30  # km/h\n",
    "    travel_times = calculate_travel_times(distance_matrix, speed)\n",
    "\n",
    "    # Set up the optimization model with weights for objectives\n",
    "    weight_profit = 0.5  # Weight for profit (maximize delivery values)\n",
    "    weight_distance = 0.5  # Weight for distance (minimize total distance)\n",
    "    \n",
    "    model, visit, arrival_time, profit_objective, distance_objective = setup_optimization_model(\n",
    "        cities, num_cities, delivery_values, travel_times, time_windows, \n",
    "        weight_profit, weight_distance)\n",
    "\n",
    "    # Display the results\n",
    "    print_results(model, cities, visit, arrival_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca88fa",
   "metadata": {},
   "source": [
    "# Logistic Reg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ed5bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pareto Front:\n",
      "Solution 1 - Delivery Value: 2800, Travel Time: 1175.7333333333327, Visit: [1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint, random, shuffle\n",
    "\n",
    "# Assuming the distance and time window functions are already defined from the previous question\n",
    "\n",
    "# Objective functions (Total Delivery Value and Total Travel Time)\n",
    "def objective_1(visit, delivery_values):\n",
    "    return sum(delivery_values[i] * visit[i] for i in range(len(visit)))\n",
    "\n",
    "def objective_2(visit, travel_times, num_cities):\n",
    "    total_time = 0\n",
    "    for i in range(num_cities):\n",
    "        for j in range(num_cities):\n",
    "            if i != j and visit[i] == 1 and visit[j] == 1:\n",
    "                total_time += travel_times[i, j] * visit[i] * visit[j]  # Travel only between visited cities\n",
    "    return total_time\n",
    "\n",
    "# Non-dominated sorting and crowding distance functions (from NSGA-II)\n",
    "def non_dominated_sorting(population, delivery_values, travel_times):\n",
    "    fronts = []\n",
    "    for i, p in enumerate(population):\n",
    "        p['dominated_by'] = set()\n",
    "        p['dominates'] = set()\n",
    "        p['rank'] = 0\n",
    "        p['crowding_distance'] = 0\n",
    "\n",
    "    # Find domination relationships\n",
    "    for i in range(len(population)):\n",
    "        for j in range(i+1, len(population)):\n",
    "            obj_1_i = objective_1(population[i]['visit'], delivery_values)\n",
    "            obj_2_i = objective_2(population[i]['visit'], travel_times, len(delivery_values))\n",
    "            obj_1_j = objective_1(population[j]['visit'], delivery_values)\n",
    "            obj_2_j = objective_2(population[j]['visit'], travel_times, len(delivery_values))\n",
    "\n",
    "            if (obj_1_i >= obj_1_j and obj_2_i >= obj_2_j):\n",
    "                population[i]['dominates'].add(j)\n",
    "                population[j]['dominated_by'].add(i)\n",
    "            elif (obj_1_j >= obj_1_i and obj_2_j >= obj_2_i):\n",
    "                population[j]['dominates'].add(i)\n",
    "                population[i]['dominated_by'].add(j)\n",
    "\n",
    "    # Create fronts\n",
    "    fronts.append([i for i in range(len(population)) if len(population[i]['dominated_by']) == 0])\n",
    "    # Continue assigning fronts\n",
    "    k = 0\n",
    "    while len(fronts[k]) > 0:\n",
    "        Q = []\n",
    "        for p in fronts[k]:\n",
    "            for q in population[p]['dominates']:\n",
    "                population[q]['dominated_by'].remove(p)\n",
    "                if len(population[q]['dominated_by']) == 0:\n",
    "                    Q.append(q)\n",
    "        k += 1\n",
    "        fronts.append(Q)\n",
    "\n",
    "    return fronts\n",
    "\n",
    "def crowding_distance(population, front, delivery_values, travel_times):\n",
    "    distance = {i: 0 for i in front}\n",
    "    for i in front:\n",
    "        obj_1_i = objective_1(population[i]['visit'], delivery_values)\n",
    "        obj_2_i = objective_2(population[i]['visit'], travel_times, len(delivery_values))\n",
    "        \n",
    "        # Calculate crowding distance here...\n",
    "        # You can add your code to calculate the crowding distance based on objective values\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Initialize Population\n",
    "def create_population(pop_size, num_cities):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        visit = [randint(0, 1) for _ in range(num_cities)]  # Random solution\n",
    "        population.append({\n",
    "            'visit': visit\n",
    "        })\n",
    "    return population\n",
    "\n",
    "# Crossover function (Two-point crossover)\n",
    "def crossover(parent1, parent2):\n",
    "    crossover_point = randint(0, len(parent1['visit']) - 1)\n",
    "    offspring1 = parent1['visit'][:crossover_point] + parent2['visit'][crossover_point:]\n",
    "    offspring2 = parent2['visit'][:crossover_point] + parent1['visit'][crossover_point:]\n",
    "    return {'visit': offspring1}, {'visit': offspring2}\n",
    "\n",
    "# Mutation function\n",
    "def mutate(offspring, mutation_rate):\n",
    "    for i in range(len(offspring['visit'])):\n",
    "        if random() < mutation_rate:\n",
    "            offspring['visit'][i] = 1 - offspring['visit'][i]  # Flip the bit (0->1 or 1->0)\n",
    "    return offspring\n",
    "\n",
    "# Selection function using tournament selection\n",
    "def tournament_selection(population, fronts, k=2):\n",
    "    front = fronts[0]  # First front (best solutions)\n",
    "    selected = []\n",
    "    for _ in range(k):\n",
    "        ind1 = population[front[randint(0, len(front)-1)]]\n",
    "        ind2 = population[front[randint(0, len(front)-1)]]\n",
    "        selected.append(ind1 if random() < 0.5 else ind2)  # Simple selection (not optimal)\n",
    "    return selected\n",
    "\n",
    "# Main loop for NSGA-II (Iterate over generations)\n",
    "def nsga_ii(pop_size, num_generations, delivery_values, travel_times, num_cities, mutation_rate=0.1):\n",
    "    population = create_population(pop_size, num_cities)\n",
    "    for gen in range(num_generations):\n",
    "        fronts = non_dominated_sorting(population, delivery_values, travel_times)\n",
    "\n",
    "        # Tournament Selection, Crossover, and Mutation\n",
    "        new_population = []\n",
    "        while len(new_population) < pop_size:\n",
    "            parents = tournament_selection(population, fronts)\n",
    "            offspring1, offspring2 = crossover(parents[0], parents[1])\n",
    "            offspring1 = mutate(offspring1, mutation_rate)\n",
    "            offspring2 = mutate(offspring2, mutation_rate)\n",
    "            new_population.append(offspring1)\n",
    "            new_population.append(offspring2)\n",
    "        \n",
    "        # Evaluate fitness (dominance sorting)\n",
    "        fronts = non_dominated_sorting(new_population, delivery_values, travel_times)\n",
    "        \n",
    "        # Perform crowding distance calculation (for replacement and diversity)\n",
    "        # Assuming we handle this in the sorting\n",
    "        population = new_population  # Replace population with new one\n",
    "    \n",
    "    # After all generations, print the results\n",
    "    fronts = non_dominated_sorting(population, delivery_values, travel_times)\n",
    "    print(\"Pareto Front:\")\n",
    "    for i in fronts[0]:  # Get first front (non-dominated solutions)\n",
    "        obj_1 = objective_1(population[i]['visit'], delivery_values)\n",
    "        obj_2 = objective_2(population[i]['visit'], travel_times, num_cities)\n",
    "        print(f\"Solution {i} - Delivery Value: {obj_1}, Travel Time: {obj_2}, Visit: {population[i]['visit']}\")\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    cities = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\"]\n",
    "    num_cities = len(cities)\n",
    "    delivery_values = [0, 500, 300, 400, 600, 200, 350, 450]\n",
    "    time_windows = [(0, 24), (9, 12), (10, 14), (11, 13), (13, 17), (8.5, 10.5), (12, 16), (14, 18)]\n",
    "\n",
    "    # Generate distance matrix and calculate travel times\n",
    "    seed = 5286\n",
    "    distance_matrix = generate_distances(num_cities, seed)\n",
    "    speed = 30  # km/h\n",
    "    travel_times = calculate_travel_times(distance_matrix, speed)\n",
    "\n",
    "    # Run NSGA-II\n",
    "    nsga_ii(pop_size=50, num_generations=100, delivery_values=delivery_values, \n",
    "            travel_times=travel_times, num_cities=num_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ad4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Breast Cancer Dataset\n",
    "file_path = 'Cancerdata.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.head()\n",
    "data.info()\n",
    "print(\"Summary Statistics\") \n",
    "data.describe()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, explained_variance_score\n",
    "\n",
    "\n",
    "# Step 1: Define independent variables (X) and target variable (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last (target)\n",
    "y = data.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Step 2: Detect and remove outliers\n",
    "# Using Z-score method for outlier detection\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(X))  # Compute the Z-scores for X\n",
    "threshold = 3  # Consider Z-scores greater than 3 as outliers\n",
    "outlier_mask = (z_scores < threshold).all(axis=1)  # Keep rows where all Z-scores are below the threshold\n",
    "\n",
    "# Apply the mask to remove outliers from the dataset\n",
    "X_clean = X[outlier_mask]\n",
    "y_clean = y[outlier_mask]\n",
    "\n",
    "# Step 3: Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Standardize the features (important for linear regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "# Print out the evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "print(f\"Explained Variance Score (EVS): {evs}\")\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Compute VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_train_scaled, i) for i in range(X_train_scaled.shape[1])]\n",
    "\n",
    "vif_data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, explained_variance_score\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Step 1: Define independent variables (X) and target variable (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last (target)\n",
    "y = data.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Step 2: Detect and remove outliers\n",
    "# Using Z-score method for outlier detection\n",
    "z_scores = np.abs(stats.zscore(X))  # Compute the Z-scores for X\n",
    "threshold = 3  # Consider Z-scores greater than 3 as outliers\n",
    "outlier_mask = (z_scores < threshold).all(axis=1)  # Keep rows where all Z-scores are below the threshold\n",
    "\n",
    "# Apply the mask to remove outliers from the dataset\n",
    "X_clean = X[outlier_mask]\n",
    "y_clean = y[outlier_mask]\n",
    "\n",
    "# Step 3: Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Standardize the features (important for linear regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Step 7: Evaluate the model using various metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "# Step 8: Print out the evaluation metrics\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "print(f\"Explained Variance Score (EVS): {evs}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Step 1: Define independent variables (X) and target variable (y)\n",
    "X = data.iloc[:, :-1]  # All columns except the last (target)\n",
    "y = data.iloc[:, -1]   # Target variable (last column)\n",
    "\n",
    "# Step 2: Detect and remove outliers\n",
    "# Using Z-score method for outlier detection\n",
    "z_scores = np.abs(stats.zscore(X))  # Compute the Z-scores for X\n",
    "threshold = 3  # Consider Z-scores greater than 3 as outliers\n",
    "outlier_mask = (z_scores < threshold).all(axis=1)  # Keep rows where all Z-scores are below the threshold\n",
    "\n",
    "# Apply the mask to remove outliers from the dataset\n",
    "X_clean = X[outlier_mask]\n",
    "y_clean = y[outlier_mask]\n",
    "\n",
    "# Step 3: Split the data into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Standardize the features (important for PCA and linear regression)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 5: Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.95)  # Choose the number of components to explain 95% of the variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Step 6: Train the Linear Regression model on the reduced dataset\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test set\n",
    "y_pred = lr_model.predict(X_test_pca)\n",
    "\n",
    "# Step 8: Evaluate the model using various metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "# Step 9: Print out the evaluation metrics\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "print(f\"Explained Variance Score (EVS): {evs}\")\n",
    "\n",
    "# Step 10: Check the significance of each independent variable using p-values\n",
    "# Add constant to X_train_pca for intercept term (this is required for statsmodels)\n",
    "X_train_with_intercept = sm.add_constant(X_train_pca)\n",
    "\n",
    "# Fit the OLS (Ordinary Least Squares) regression model using statsmodels\n",
    "ols_model = sm.OLS(y_train, X_train_with_intercept).fit()\n",
    "\n",
    "# Print the summary of the OLS model to get p-values and other statistics\n",
    "print(\"\\nOLS Regression Summary:\")\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Step 11: Determine significance at α = 0.05\n",
    "# Extract the p-values from the model summary\n",
    "p_values = ols_model.pvalues[1:]  # Skip the intercept (first entry)\n",
    "\n",
    "# Print the p-values and determine significance\n",
    "for i, p_value in enumerate(p_values):\n",
    "    feature = f\"Principal Component {i+1}\"  # Corresponding feature in PCA\n",
    "    if p_value < 0.05:\n",
    "        print(f\"{feature} is significant (p-value: {p_value})\")\n",
    "    else:\n",
    "        print(f\"{feature} is not significant (p-value: {p_value})\")\n",
    "\n",
    "\n",
    "# Step 1: Fit the Logistic Regression model using sklearn (if not already done)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "log_reg_model = LogisticRegression(solver='liblinear', penalty='l2')\n",
    "log_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 2: Extract the coefficients and calculate odds ratios\n",
    "coefficients = log_reg_model.coef_[0]  # Coefficients for each feature\n",
    "odds_ratios = np.exp(coefficients)  # Exponentiate to get the odds ratios\n",
    "\n",
    "chosen_variables = X_train.columns[:5]\n",
    "\n",
    "# Create a dictionary of the odds ratios for the chosen variables\n",
    "odds_ratio_dict = dict(zip(chosen_variables, odds_ratios[:5]))\n",
    "\n",
    "# Step 4: Display and interpret the odds ratios\n",
    "print(\"\\nOdds Ratios and Interpretation:\")\n",
    "for var, or_value in odds_ratio_dict.items():\n",
    "    print(f\"Odds ratio for {var}: {or_value:.2f}\")\n",
    "    \n",
    "    if or_value > 1:\n",
    "        print(f\"Interpretation: A one-unit increase in {var} increases the odds of the outcome by a factor of {or_value:.2f}.\")\n",
    "    elif or_value < 1:\n",
    "        print(f\"Interpretation: A one-unit increase in {var} decreases the odds of the outcome by a factor of {1/or_value:.2f}.\")\n",
    "    else:\n",
    "        print(f\"Interpretation: {var} has no effect on the odds of the outcome.\")\n",
    "\n",
    "\n",
    "X = data[['mean radius', 'mean texture', 'mean smoothness', 'mean symmetry']]\n",
    "y = data['target']  # Or whatever the target variable is\n",
    "\n",
    "# Step 2: Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 3: Standardize the data (fit scaler on the training data only)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg_model = LogisticRegression()\n",
    "log_reg_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "input_features = np.array([[14.5, 18.0, 0.095, 0.180]])  # New patient's tumor features\n",
    "input_features_scaled = scaler.transform(input_features)\n",
    "predicted_prob = log_reg_model.predict_proba(input_features_scaled)[0, 1]  # Probability for class 1 (malignant)\n",
    "\n",
    "# Step 6: Display prediction\n",
    "print(f\"Predicted Probability of Malignancy: {predicted_prob:.4f}\")\n",
    "if predicted_prob > 0.5:\n",
    "    print(\"Prediction: Malignant\")\n",
    "else:\n",
    "    print(\"Prediction: Benign\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1a214",
   "metadata": {},
   "source": [
    "# time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "file_path = \"time-series.csv\"  # Path to the uploaded CSV file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Extract the 'V6' column for analysis\n",
    "time_series_cleaned = data['V6']\n",
    "\n",
    "# Step 3: Plot the raw time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_series_cleaned, marker='o', label='V6 (Sales)')\n",
    "plt.title('Raw Time Series (V6)')\n",
    "plt.xlabel('Time (Quarters)')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Plot ACF and PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Autocorrelation Function (ACF) plot\n",
    "plot_acf(time_series_cleaned, lags=20, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation (ACF)')\n",
    "\n",
    "# Partial Autocorrelation Function (PACF) plot\n",
    "plot_pacf(time_series_cleaned, lags=20, ax=axes[1])\n",
    "axes[1].set_title('Partial Autocorrelation (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Detect outliers using Z-scores\n",
    "z_scores = np.abs((time_series_cleaned - time_series_cleaned.mean()) / time_series_cleaned.std())\n",
    "outliers = time_series_cleaned[z_scores > 3]  # Threshold of 3 for detecting outliers\n",
    "\n",
    "# Print outliers\n",
    "if not outliers.empty:\n",
    "    print(\"Outliers detected:\")\n",
    "    print(outliers)\n",
    "else:\n",
    "    print(\"No outliers detected.\")\n",
    "\n",
    "# Replace outliers with the median value\n",
    "median_value = time_series.median()\n",
    "time_series_cleaned = time_series.copy()\n",
    "time_series_cleaned[z_scores > 3] = median_value\n",
    "\n",
    "# Plot the cleaned time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_series_cleaned, label='Cleaned V6 (Sales)', marker='o')\n",
    "plt.title('Cleaned Time Series (V6)')\n",
    "plt.xlabel('Time (Quarters)')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform the Augmented Dickey-Fuller (ADF) test\n",
    "adf_result = adfuller(time_series_cleaned)\n",
    "\n",
    "# Display the ADF statistic and p-value\n",
    "print(f\"ADF Statistic: {adf_result[0]:.4f}\")\n",
    "print(f\"p-value: {adf_result[1]:.4f}\")\n",
    "\n",
    "# Interpret the test results\n",
    "if adf_result[1] < 0.05:\n",
    "    print(\"The time series is stationary.\")\n",
    "else:\n",
    "    print(\"The time series is not stationary; further transformations are required.\")\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  \n",
    "arima_model = ARIMA(time_series_cleaned, order=(1, 1, 1))  # Example order\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "\n",
    "print(arima_result.summary())\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, acorr_ljungbox\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Plot the residuals\n",
    "residuals = arima_result.resid\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(residuals, marker='o', label='Residuals', color='blue')\n",
    "plt.axhline(0, linestyle='--', color='red')\n",
    "plt.title('Residuals of ARIMA Model', color='darkgreen')\n",
    "plt.xlabel('Time (Quarters)', color='darkblue')\n",
    "plt.ylabel('Residuals', color='darkblue')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Plot ACF of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(residuals, lags=20, color='purple')\n",
    "plt.title('ACF of Residuals', color='darkgreen')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Histogram of residuals to check normality\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, color='orange')\n",
    "plt.title('Histogram of Residuals', color='darkgreen')\n",
    "plt.xlabel('Residual Values', color='darkblue')\n",
    "plt.ylabel('Frequency', color='darkblue')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Perform Ljung-Box test for autocorrelation\n",
    "ljung_box_test = acorr_ljungbox(residuals, lags=20)\n",
    "print(\"Ljung-Box Test Results:\")\n",
    "print(ljung_box_test)\n",
    "\n",
    "# Step 5: Perform Jarque-Bera test for normality\n",
    "jb_test = stats.jarque_bera(residuals)\n",
    "print(\"Jarque-Bera Test Results:\")\n",
    "print(f\"JB Statistic: {jb_test[0]:.4f}, p-value: {jb_test[1]:.4f}\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "# Calculate Z-scores for the 'V6' column\n",
    "z_scores = zscore(data['V6'])\n",
    "\n",
    "# Identify outliers based on Z-score threshold (e.g., abs(z) > 3)\n",
    "outliers = np.where(np.abs(z_scores) > 3)\n",
    "\n",
    "# Print the outlier indices and values\n",
    "print(f\"Outliers detected at indices: {outliers[0]}\")\n",
    "print(f\"Outlier values: {data.iloc[outliers[0]]}\")\n",
    "\n",
    "# Remove outliers by replacing them with NaN\n",
    "data_cleaned = data.copy()\n",
    "data_cleaned.iloc[outliers[0], data_cleaned.columns.get_loc('V6')] = np.nan\n",
    "\n",
    "# Replace NaN values in the 'V6' column with the median\n",
    "data_cleaned['V6'].fillna(data_cleaned['V6'].median(), inplace=True)\n",
    "\n",
    "# Plot the original and cleaned data for comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['V6'], label='Original Data', color='blue', alpha=0.6)\n",
    "plt.plot(data_cleaned['V6'], label='Data after Outlier Removal', color='red', alpha=0.6)\n",
    "plt.title('Time Series with Outliers Removed')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure data_cleaned is numeric and handle missing values\n",
    "data_cleaned = pd.to_numeric(data_cleaned, errors='coerce')  # Convert to numeric, coercing errors\n",
    "data_cleaned = data_cleaned.dropna()  # Drop NaNs (or use fillna() if needed)\n",
    "\n",
    "# Train-test split for model evaluation (80% train, 20% test)\n",
    "train_size = int(len(data_cleaned) * 0.8)\n",
    "train, test = data_cleaned[:train_size], data_cleaned[train_size:]\n",
    "\n",
    "# Initialize containers for evaluation metrics\n",
    "models = ['ARIMA', 'SARIMA']\n",
    "aic_values = []\n",
    "bic_values = []\n",
    "mse_values = []\n",
    "r2_values = []\n",
    "model_predictions = {}\n",
    "\n",
    "# --- ARIMA Model ---\n",
    "print(\"Fitting ARIMA Model...\")\n",
    "arima_model = ARIMA(train, order=(1, 1, 1))  # Example order (p=1, d=1, q=1)\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# Forecasting the test set\n",
    "arima_forecast = arima_result.forecast(steps=len(test))\n",
    "\n",
    "# Evaluate ARIMA\n",
    "arima_mse = mean_squared_error(test, arima_forecast)\n",
    "arima_r2 = r2_score(test, arima_forecast)\n",
    "aic_values.append(arima_result.aic)\n",
    "bic_values.append(arima_result.bic)\n",
    "mse_values.append(arima_mse)\n",
    "r2_values.append(arima_r2)\n",
    "model_predictions['ARIMA'] = arima_forecast\n",
    "\n",
    "# --- SARIMA Model ---\n",
    "print(\"Fitting SARIMA Model...\")\n",
    "sarima_model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 4))  # Example SARIMA model\n",
    "sarima_result = sarima_model.fit()\n",
    "\n",
    "# Forecasting the test set\n",
    "sarima_forecast = sarima_result.forecast(steps=len(test))\n",
    "\n",
    "# Evaluate SARIMA\n",
    "sarima_mse = mean_squared_error(test, sarima_forecast)\n",
    "sarima_r2 = r2_score(test, sarima_forecast)\n",
    "aic_values.append(sarima_result.aic)\n",
    "bic_values.append(sarima_result.bic)\n",
    "mse_values.append(sarima_mse)\n",
    "r2_values.append(sarima_r2)\n",
    "model_predictions['SARIMA'] = sarima_forecast\n",
    "\n",
    "# --- Additional Model (Optional: Example with ARMA or other variations) ---\n",
    "# You can add other models here, like ARMA, Exponential Smoothing, etc.\n",
    "# For simplicity, we skip this step in this example\n",
    "\n",
    "# --- Compare Models ---\n",
    "print(\"\\nModel Comparison:\")\n",
    "\n",
    "# Display model evaluation metrics (AIC, BIC, MSE, R^2)\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'AIC': aic_values,\n",
    "    'BIC': bic_values,\n",
    "    'MSE': mse_values,\n",
    "    'R^2': r2_values\n",
    "})\n",
    "\n",
    "print(model_comparison)\n",
    "\n",
    "# --- Residual Analysis ---\n",
    "# ARIMA Residuals\n",
    "arima_residuals = arima_result.resid\n",
    "\n",
    "# SARIMA Residuals\n",
    "sarima_residuals = sarima_result.resid\n",
    "\n",
    "# Plotting the actual vs predicted values for both models\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Actual vs Predicted for ARIMA\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(test.index, test, label='Actual', color='blue')\n",
    "plt.plot(test.index, arima_forecast, label='ARIMA Predicted', color='red')\n",
    "plt.title('ARIMA Model: Actual vs Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Actual vs Predicted for SARIMA\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test.index, test, label='Actual', color='blue')\n",
    "plt.plot(test.index, sarima_forecast, label='SARIMA Predicted', color='green')\n",
    "plt.title('SARIMA Model: Actual vs Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot ARIMA residuals\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(arima_residuals)\n",
    "plt.title('ARIMA Residuals')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "# Plot SARIMA residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(sarima_residuals)\n",
    "plt.title('SARIMA Residuals')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Final Conclusion ---\n",
    "# Based on the AIC, BIC, MSE, and R² values, the best model can be selected\n",
    "best_model = model_comparison.loc[model_comparison['MSE'].idxmin()]['Model']\n",
    "print(f\"\\nBest Model: {best_model}\")\n",
    "\n",
    "# --- Best Model Selection ---\n",
    "best_model_name = model_comparison.loc[model_comparison['MSE'].idxmin()]['Model']\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# --- Forecasting for the Next Four Quarters ---\n",
    "forecast_steps = 4  # Forecasting for the next four quarters\n",
    "\n",
    "# ARIMA Forecasting\n",
    "if best_model_name == 'ARIMA':\n",
    "    print(\"Using ARIMA for forecasting...\")\n",
    "    arima_forecast = arima_result.forecast(steps=forecast_steps)\n",
    "\n",
    "    # Plotting the forecasted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test.index, test, label='Actual', color='blue')\n",
    "    plt.plot(pd.date_range(test.index[-1], periods=forecast_steps, freq='Q'), arima_forecast, label='ARIMA Forecast', color='red')\n",
    "    plt.title('ARIMA Model Forecast for Next Four Quarters')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# SARIMA Forecasting\n",
    "elif best_model_name == 'SARIMA':\n",
    "    print(\"Using SARIMA for forecasting...\")\n",
    "    sarima_forecast = sarima_result.forecast(steps=forecast_steps)\n",
    "\n",
    "    # Plotting the forecasted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test.index, test, label='Actual', color='blue')\n",
    "    plt.plot(pd.date_range(test.index[-1], periods=forecast_steps, freq='Q'), sarima_forecast, label='SARIMA Forecast', color='green')\n",
    "    plt.title('SARIMA Model Forecast for Next Four Quarters')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65d0a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
